# 翻译评估器项目结构梳理

## 📋 项目概述

**项目名称**: Translation Evaluator（翻译质量评估器）  
**项目类型**: Python包 + HTTP API服务  
**主要功能**: 提供统一的翻译质量评估接口，支持多种专业评估指标

---

## 🏗️ 项目架构

### 整体架构

```
┌─────────────────────────────────────────────────────────┐
│                    HTTP API 服务层                        │
│              (eval_server.py - Flask)                   │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│              UnifiedEvaluator (统一评估器)                │
│         集成所有6个指标，提供论文级综合评分                │
└────────────────────┬────────────────────────────────────┘
                     │
                     ▼
┌─────────────────────────────────────────────────────────┐
│         CombinedQualityScorer (组合评估器)                │
│    整合神经网络模型(COMET/BLEURT/BERTScore)和MQM          │
└─────┬──────────┬──────────┬──────────┬──────────────────┘
      │          │          │          │
      ▼          ▼          ▼          ▼
┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐
│ COMET   │ │ BLEURT  │ │BERTScore│ │  ChrF   │
│ Scorer  │ │ Scorer  │ │ Scorer  │ │ Scorer  │
└─────────┘ └─────────┘ └─────────┘ └─────────┘
```

### 运行模式

1. **库模式**: 作为Python包导入使用
2. **API服务模式**: 作为独立的HTTP服务运行（推荐用于生产环境）

---

## 📁 目录结构详解

```
translation_evaluator/
├── translation_evaluator/          # 核心评估库
│   ├── __init__.py                 # 模块入口，导出公共API
│   ├── unified_evaluator.py        # 统一评估器（顶层）
│   ├── combined_scorer.py          # 组合评估器（中层）
│   ├── comet_scorer.py             # COMET评估器
│   ├── bleurt_scorer.py            # BLEURT评估器
│   ├── bertscore_scorer.py         # BERTScore评估器
│   └── chrf_scorer.py              # ChrF评估器
│
├── eval_server.py                  # API服务器（Flask）
├── eval_client.py                  # API客户端示例
│
├── BLEURT-20/                      # BLEURT模型文件（约500MB）
│   ├── bert_config.json
│   ├── bleurt_config.json
│   ├── saved_model.pb
│   ├── sent_piece.model
│   ├── sent_piece.vocab
│   └── variables/                  # 模型权重
│
├── logs/                           # API请求日志
│   └── api_YYYYMMDD.log            # 按日期命名的日志文件
│
├── setup.py                        # 包配置和依赖管理
├── start_server.sh                 # Linux/Mac启动脚本
├── start_server.bat                # Windows启动脚本
│
├── README.md                       # 项目文档
├── API服务使用指南.md              # API使用文档
├── 项目结构分析.md                 # 技术架构分析
│
└── test_*.py                       # 测试文件
    ├── test_evaluator.py           # 评估器功能测试
    ├── test_api.py                 # API接口测试
    └── test_api_call.py            # API调用测试
```

---

## 🔧 核心模块说明

### 1. 评估器模块（Scorers）

#### `unified_evaluator.py`
- **类**: `UnifiedEvaluator`, `PaperGradeScore`
- **功能**: 顶层统一评估器，集成所有6个指标
- **权重分配**: COMET(25%), BERTScore(20%), BLEURT(15%), MQM(20%), BLEU(10%), ChrF(10%)

#### `combined_scorer.py`
- **类**: `CombinedQualityScorer`, `ComprehensiveScore`
- **功能**: 组合评估器，整合神经网络模型和MQM
- **特点**: 权重自适应，根据可用模型动态调整

#### `comet_scorer.py`
- **类**: `COMETScorer`, `COMETKiwiScorer`
- **模型**: `Unbabel/wmt22-comet-da` (默认)
- **依赖**: `unbabel-comet>=2.0.0`
- **特点**: WMT官方推荐，需要参考翻译

#### `bleurt_scorer.py`
- **类**: `BLEURTScorer`
- **模型**: `BLEURT-20` (本地模型文件)
- **依赖**: `bleurt>=0.0.1`, `tensorflow`
- **特点**: 支持自动下载模型，Google开发

#### `bertscore_scorer.py`
- **类**: `BERTScoreScorer`
- **依赖**: `bert-score>=0.3.13`
- **特点**: 基于BERT embedding的语义相似度

#### `chrf_scorer.py`
- **类**: `ChrFScorer`, `ChrF1Scorer`, `ChrF2Scorer`, `ChrF3Scorer`
- **依赖**: `sacrebleu>=2.0.0`
- **特点**: 对形态变化丰富的语言友好

### 2. 服务层模块

#### `eval_server.py`
- **框架**: Flask
- **功能**: HTTP API服务器
- **接口**:
  - `GET /` - API信息
  - `GET /health` - 健康检查
  - `POST /eval` - 单个样本评估
  - `POST /eval/batch` - 批量评估
- **特性**:
  - CORS支持
  - 详细日志记录
  - 错误处理
  - 可配置评估器

#### `eval_client.py`
- **类**: `EvaluationClient`
- **功能**: API客户端封装
- **用途**: 示例代码，展示如何调用API

---

## 📦 依赖关系

### 核心依赖（必需）
```python
numpy>=1.20.0
flask>=2.0.0
flask-cors>=3.0.0
requests>=2.25.0
```

### 可选依赖（按需安装）
```python
# BERTScore
bert-score>=0.3.13

# COMET
unbabel-comet>=2.0.0
pytorch  # COMET依赖

# BLEURT
bleurt>=0.0.1
tensorflow  # BLEURT依赖

# ChrF
sacrebleu>=2.0.0
```

### 特殊依赖说明

1. **PyTorch**: COMET需要，建议使用conda安装
2. **TensorFlow**: BLEURT需要，版本兼容性需注意
3. **NumPy**: 版本冲突问题，建议固定版本（1.23.5）

---

## 🎯 支持的评估指标

| 指标 | 类型 | 依赖 | 模型大小 | 特点 |
|------|------|------|----------|------|
| **BLEU** | 传统n-gram | 无 | - | 快速，基础指标 |
| **COMET** | 神经网络 | PyTorch | ~500MB | WMT官方推荐 |
| **BLEURT** | 神经网络 | TensorFlow | ~500MB | Google开发 |
| **BERTScore** | 语义相似度 | PyTorch | ~400MB | 基于BERT |
| **ChrF** | 字符n-gram | sacrebleu | - | 对中文友好 |
| **MQM** | 人工评估 | 无 | - | 外部输入 |

---

## 🔄 数据流

### API请求流程

```
客户端请求
  │
  ▼
eval_server.py (Flask)
  │
  ├─→ 请求验证
  ├─→ 日志记录
  │
  ▼
UnifiedEvaluator.score()
  │
  ├─→ CombinedQualityScorer.score()
  │     ├─→ COMETScorer.score_single()
  │     ├─→ BLEURTScorer.score_single()
  │     ├─→ BERTScoreScorer.score_single()
  │     ├─→ ChrFScorer.score_single()
  │     └─→ _calculate_bleu()
  │
  └─→ _calculate_paper_grade_score()
        │
        └─→ 加权平均计算综合评分
  │
  ▼
返回JSON响应
```

---

## 📊 配置说明

### 评估器配置（eval_server.py）

```python
evaluator_config = {
    "use_bleu": True,
    "use_comet": True,
    "use_bleurt": True,  # 默认关闭，需要TensorFlow
    "use_bertscore": True,
    "use_mqm": True,
    "use_chrf": True
}
```

### 服务器配置

- **默认端口**: 5001
- **默认地址**: 0.0.0.0（监听所有接口）
- **日志目录**: `logs/`
- **日志格式**: 按日期命名 `api_YYYYMMDD.log`

---

## 🚀 启动方式

### 1. 直接运行
```bash
python eval_server.py
```

### 2. 使用启动脚本（Linux/Mac）
```bash
chmod +x start_server.sh
./start_server.sh
```

### 3. 使用启动脚本（Windows）
```cmd
start_server.bat
```

### 4. 带参数启动
```bash
python eval_server.py --port 5001 --use-bleurt --debug
```

**启动参数**:
- `--host`: 监听地址（默认: 0.0.0.0）
- `--port`: 监听端口（默认: 5001）
- `--debug`: 启用Flask调试模式
- `--use-bleurt`: 启用BLEURT评估器
- `--no-api-debug`: 禁用API请求调试日志

---

## 📝 关键文件说明

### 配置文件
- `setup.py`: Python包配置，定义依赖和可选安装项
- `start_server.sh`: Linux/Mac启动脚本
- `start_server.bat`: Windows启动脚本

### 文档文件
- `README.md`: 项目主文档
- `API服务使用指南.md`: API使用详细说明
- `项目结构分析.md`: 技术架构深度分析

### 测试文件
- `test_evaluator.py`: 评估器功能测试
- `test_api.py`: API接口测试
- `test_api_call.py`: API调用示例

---

## 🔐 安全特性

1. **CORS支持**: 允许跨域请求（可配置）
2. **错误处理**: 完善的异常捕获和错误返回
3. **请求验证**: 验证必需字段
4. **日志记录**: 详细记录所有请求和响应

---

## 📈 性能特点

1. **模型预加载**: 启动时加载模型到内存
2. **批量处理**: 支持批量评估接口
3. **并发支持**: Flask多线程处理
4. **延迟初始化**: 模型按需加载

---

## 🎯 使用场景

1. **翻译质量评估**: 评估翻译模型的输出质量
2. **论文实验**: 用于学术研究的评估指标
3. **生产监控**: 实时监控翻译质量
4. **API服务**: 作为独立服务供其他系统调用

---

## 📌 总结

这是一个**设计良好的模块化评估系统**，具有以下特点：

✅ **清晰的层次结构**: 统一评估器 → 组合评估器 → 各评估器  
✅ **灵活的配置选项**: 可选择启用/禁用特定评估器  
✅ **容错机制完善**: 某个模型加载失败不影响其他模型  
✅ **易于扩展**: 支持添加新的评估器  
✅ **多种运行模式**: 库模式 + API服务模式  
✅ **完善的文档**: 详细的使用说明和技术文档

适合用于翻译质量评估、论文实验和生产环境质量监控。

