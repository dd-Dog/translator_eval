"""
ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨
æä¾›HTTP APIæ¥å£ï¼Œæ”¯æŒç‹¬ç«‹è¿è¡Œ
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import traceback
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from translation_evaluator import UnifiedEvaluator, PaperGradeScore

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€è¯„ä¼°å™¨å®ä¾‹å’Œé…ç½®
evaluator = None
evaluator_config = {
    "use_bleu": True,
    "use_comet": True,
    "use_bleurt": True,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦TensorFlow
    "use_bertscore": True,
    "use_mqm": True,
    "use_chrf": True
}


def init_evaluator(use_bleurt=None, force_reinit=False):
    """
    åˆå§‹åŒ–è¯„ä¼°å™¨
    
    Args:
        use_bleurt: æ˜¯å¦ä½¿ç”¨BLEURTï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        force_reinit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ï¼ˆå³ä½¿å·²åˆå§‹åŒ–ï¼‰
    """
    global evaluator, evaluator_config
    
    # å¦‚æœæŒ‡å®šäº†use_bleurtï¼Œæ›´æ–°é…ç½®
    if use_bleurt is not None:
        evaluator_config["use_bleurt"] = use_bleurt
        # å¦‚æœé…ç½®æ”¹å˜ä¸”è¯„ä¼°å™¨å·²åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
        if evaluator is not None:
            force_reinit = True
    
    if evaluator is None or force_reinit:
        if force_reinit and evaluator is not None:
            print("âš ï¸  æ£€æµ‹åˆ°é…ç½®å˜æ›´ï¼Œé‡æ–°åˆå§‹åŒ–è¯„ä¼°å™¨...")
            evaluator = None
        print("=" * 80)
        print("åˆå§‹åŒ–ç¿»è¯‘è¯„ä¼°å™¨...")
        print("=" * 80)
        
        if evaluator_config["use_bleurt"]:
            print("âš ï¸  å¯ç”¨BLEURTè¯„ä¼°å™¨ï¼ˆéœ€è¦TensorFlowå’Œæ¨¡å‹æ–‡ä»¶ï¼‰")
        
        evaluator = UnifiedEvaluator(
            use_bleu=evaluator_config["use_bleu"],
            use_comet=evaluator_config["use_comet"],
            use_bleurt=evaluator_config["use_bleurt"],
            use_bertscore=evaluator_config["use_bertscore"],
            use_mqm=evaluator_config["use_mqm"],
            use_chrf=evaluator_config["use_chrf"]
        )
        
        success = evaluator.initialize()
        
        # æ˜¾ç¤ºå®é™…å¯ç”¨çš„è¯„ä¼°å™¨çŠ¶æ€
        print("\n" + "=" * 80)
        print("è¯„ä¼°å™¨çŠ¶æ€:")
        print("=" * 80)
        
        enabled = []
        failed = []
        
        if evaluator_config["use_bleu"]:
            enabled.append("BLEU")
        
        if evaluator_config["use_comet"]:
            if evaluator.use_comet and evaluator.comet_scorer:
                enabled.append("COMET âœ…")
            else:
                failed.append("COMET âŒ")
        
        if evaluator_config["use_bleurt"]:
            if evaluator.use_bleurt and evaluator.bleurt_scorer:
                enabled.append("BLEURT âœ…")
            else:
                failed.append("BLEURT âŒ (å¯èƒ½ç¼ºå°‘TensorFlowæˆ–æ¨¡å‹æ–‡ä»¶)")
        
        if evaluator_config["use_bertscore"]:
            if evaluator.use_bertscore and evaluator.bertscore_scorer:
                enabled.append("BERTScore âœ…")
            else:
                failed.append("BERTScore âŒ")
        
        if evaluator_config["use_chrf"]:
            if evaluator.use_chrf and evaluator.chrf_scorer:
                enabled.append("ChrF âœ…")
            else:
                failed.append("ChrF âŒ")
        
        if evaluator_config["use_mqm"]:
            enabled.append("MQM")
        
        if enabled:
            print(f"âœ… å·²å¯ç”¨: {', '.join(enabled)}")
        if failed:
            print(f"âš ï¸  åˆå§‹åŒ–å¤±è´¥: {', '.join(failed)}")
        
        if success:
            print("\nâœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½†æœåŠ¡ä»å¯è¿è¡Œ")
        
        print("=" * 80)
        print("APIæœåŠ¡å·²å°±ç»ª")
        print("=" * 80)
    
    return evaluator


@app.route("/", methods=["GET"])
def index():
    """APIé¦–é¡µ"""
    return jsonify({
        "service": "Translation Evaluator API",
        "version": "1.0.0",
        "endpoints": {
            "/": "APIä¿¡æ¯",
            "/health": "å¥åº·æ£€æŸ¥",
            "/eval": "å•ä¸ªæ ·æœ¬è¯„ä¼° (POST)",
            "/eval/batch": "æ‰¹é‡è¯„ä¼° (POST)"
        },
        "usage": {
            "single": {
                "url": "/eval",
                "method": "POST",
                "body": {
                    "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                    "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
                    "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
                    "mqm_score": "MQMè¯„åˆ†ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "batch": {
                "url": "/eval/batch",
                "method": "POST",
                "body": {
                    "sources": ["æºæ–‡æœ¬åˆ—è¡¨"],
                    "translations": ["ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨"],
                    "references": ["å‚è€ƒç¿»è¯‘åˆ—è¡¨"],
                    "mqm_scores": ["MQMè¯„åˆ†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰"]
                }
            }
        }
    })


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    evaluator_status = {}
    if evaluator is not None:
        evaluator_status = {
            "use_bleu": evaluator.use_bleu,
            "use_comet": evaluator.use_comet and evaluator.comet_scorer is not None,
            "use_bleurt": evaluator.use_bleurt and evaluator.bleurt_scorer is not None,
            "use_bertscore": evaluator.use_bertscore and evaluator.bertscore_scorer is not None,
            "use_chrf": evaluator.use_chrf and evaluator.chrf_scorer is not None,
            "use_mqm": evaluator.use_mqm
        }
    
    return jsonify({
        "status": "healthy",
        "evaluator_initialized": evaluator is not None,
        "evaluator_status": evaluator_status
    })


@app.route("/eval", methods=["POST"])
def eval_text():
    """
    å•ä¸ªæ ·æœ¬è¯„ä¼°
    
    Request Body:
    {
        "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
        "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
        "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
        "mqm_score": {
            "adequacy": 0.9,
            "fluency": 0.85,
            "terminology": 0.95,
            "overall": 0.9
        }  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "score": {
            "bleu": 0.85,
            "comet": 0.92,
            "bleurt": 0.0,
            "bertscore_f1": 0.88,
            "chrf": 0.87,
            "mqm_adequacy": 0.9,
            "mqm_fluency": 0.85,
            "mqm_terminology": 0.95,
            "mqm_overall": 0.9,
            "final_score": 0.89
        }
    }
    """
    try:
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translation" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translation"
            }), 400
        
        if "reference" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: reference"
            }), 400
        
        # æ‰§è¡Œè¯„ä¼°
        score = evaluator.score(
            source=data.get("source", ""),
            translation=data["translation"],
            reference=data["reference"],
            mqm_score=data.get("mqm_score")
        )
        
        # è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¤„ç†dataclassï¼‰
        if isinstance(score, PaperGradeScore):
            score_dict = {
                "bleu": score.bleu,
                "comet": score.comet,
                "bleurt": score.bleurt,
                "bertscore_f1": score.bertscore_f1,
                "chrf": score.chrf,
                "mqm_adequacy": score.mqm_adequacy,
                "mqm_fluency": score.mqm_fluency,
                "mqm_terminology": score.mqm_terminology,
                "mqm_overall": score.mqm_overall,
                "final_score": score.final_score,
                "model_info": score.model_info
            }
        else:
            score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
        
        return jsonify({
            "success": True,
            "score": score_dict
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        print(f"è¯„ä¼°é”™è¯¯: {error_msg}")
        print(traceback_str)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


@app.route("/eval/batch", methods=["POST"])
def eval_batch():
    """
    æ‰¹é‡è¯„ä¼°
    
    Request Body:
    {
        "sources": ["æºæ–‡æœ¬1", "æºæ–‡æœ¬2", ...],
        "translations": ["ç¿»è¯‘1", "ç¿»è¯‘2", ...],
        "references": ["å‚è€ƒ1", "å‚è€ƒ2", ...],
        "mqm_scores": [
            {"overall": 0.9},
            {"overall": 0.85}
        ]  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "scores": [
            {
                "bleu": 0.85,
                "comet": 0.92,
                ...
            },
            ...
        ]
    }
    """
    try:
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translations" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translations"
            }), 400
        
        if "references" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: references"
            }), 400
        
        translations = data["translations"]
        references = data["references"]
        sources = data.get("sources", [""] * len(translations))
        mqm_scores = data.get("mqm_scores", [None] * len(translations))
        
        # éªŒè¯é•¿åº¦
        if len(translations) != len(references):
            return jsonify({
                "success": False,
                "error": f"translationså’Œreferencesé•¿åº¦ä¸åŒ¹é…: {len(translations)} vs {len(references)}"
            }), 400
        
        # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
        results = evaluator.batch_score(
            sources=sources,
            translations=translations,
            references=references,
            mqm_scores=mqm_scores if mqm_scores else None
        )
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        scores_list = []
        for score in results:
            if isinstance(score, PaperGradeScore):
                score_dict = {
                    "bleu": score.bleu,
                    "comet": score.comet,
                    "bleurt": score.bleurt,
                    "bertscore_f1": score.bertscore_f1,
                    "chrf": score.chrf,
                    "mqm_adequacy": score.mqm_adequacy,
                    "mqm_fluency": score.mqm_fluency,
                    "mqm_terminology": score.mqm_terminology,
                    "mqm_overall": score.mqm_overall,
                    "final_score": score.final_score
                }
            else:
                score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
            scores_list.append(score_dict)
        
        return jsonify({
            "success": True,
            "count": len(scores_list),
            "scores": scores_list
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        print(f"æ‰¹é‡è¯„ä¼°é”™è¯¯: {error_msg}")
        print(traceback_str)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--port", type=int, default=5001, help="ç›‘å¬ç«¯å£ (é»˜è®¤: 5001)")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--use-bleurt", action="store_true", help="å¯ç”¨BLEURTè¯„ä¼°å™¨")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä¼ é€’use_bleurtå‚æ•°ï¼‰
    init_evaluator(use_bleurt=args.use_bleurt)
    
    print(f"\nğŸš€ å¯åŠ¨APIæœåŠ¡å™¨...")
    print(f"   åœ°å€: http://{args.host}:{args.port}")
    print(f"   è°ƒè¯•æ¨¡å¼: {args.debug}")
    print(f"\nğŸ“– APIæ–‡æ¡£: http://{args.host}:{args.port}/")
    print(f"ğŸ’š å¥åº·æ£€æŸ¥: http://{args.host}:{args.port}/health")
    print(f"ğŸ“Š è¯„ä¼°æ¥å£: http://{args.host}:{args.port}/eval")
    print(f"ğŸ“¦ æ‰¹é‡è¯„ä¼°: http://{args.host}:{args.port}/eval/batch")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    app.run(host=args.host, port=args.port, debug=args.debug)

