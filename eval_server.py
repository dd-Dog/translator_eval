"""
ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨
æä¾›HTTP APIæ¥å£ï¼Œæ”¯æŒç‹¬ç«‹è¿è¡Œ
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import traceback
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from translation_evaluator import UnifiedEvaluator, PaperGradeScore

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# å…¨å±€è¯„ä¼°å™¨å®ä¾‹
evaluator = None


def init_evaluator():
    """åˆå§‹åŒ–è¯„ä¼°å™¨"""
    global evaluator
    if evaluator is None:
        print("=" * 80)
        print("åˆå§‹åŒ–ç¿»è¯‘è¯„ä¼°å™¨...")
        print("=" * 80)
        
        evaluator = UnifiedEvaluator(
            use_bleu=True,
            use_comet=True,
            use_bleurt=False,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦TensorFlow
            use_bertscore=True,
            use_mqm=True,
            use_chrf=True
        )
        
        success = evaluator.initialize()
        if success:
            print("\nâœ… è¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½†æœåŠ¡ä»å¯è¿è¡Œ")
        
        print("=" * 80)
        print("APIæœåŠ¡å·²å°±ç»ª")
        print("=" * 80)
    
    return evaluator


@app.route("/", methods=["GET"])
def index():
    """APIé¦–é¡µ"""
    return jsonify({
        "service": "Translation Evaluator API",
        "version": "1.0.0",
        "endpoints": {
            "/": "APIä¿¡æ¯",
            "/health": "å¥åº·æ£€æŸ¥",
            "/eval": "å•ä¸ªæ ·æœ¬è¯„ä¼° (POST)",
            "/eval/batch": "æ‰¹é‡è¯„ä¼° (POST)"
        },
        "usage": {
            "single": {
                "url": "/eval",
                "method": "POST",
                "body": {
                    "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                    "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
                    "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
                    "mqm_score": "MQMè¯„åˆ†ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "batch": {
                "url": "/eval/batch",
                "method": "POST",
                "body": {
                    "sources": ["æºæ–‡æœ¬åˆ—è¡¨"],
                    "translations": ["ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨"],
                    "references": ["å‚è€ƒç¿»è¯‘åˆ—è¡¨"],
                    "mqm_scores": ["MQMè¯„åˆ†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰"]
                }
            }
        }
    })


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "healthy",
        "evaluator_initialized": evaluator is not None
    })


@app.route("/eval", methods=["POST"])
def eval_text():
    """
    å•ä¸ªæ ·æœ¬è¯„ä¼°
    
    Request Body:
    {
        "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
        "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
        "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
        "mqm_score": {
            "adequacy": 0.9,
            "fluency": 0.85,
            "terminology": 0.95,
            "overall": 0.9
        }  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "score": {
            "bleu": 0.85,
            "comet": 0.92,
            "bleurt": 0.0,
            "bertscore_f1": 0.88,
            "chrf": 0.87,
            "mqm_adequacy": 0.9,
            "mqm_fluency": 0.85,
            "mqm_terminology": 0.95,
            "mqm_overall": 0.9,
            "final_score": 0.89
        }
    }
    """
    try:
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translation" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translation"
            }), 400
        
        if "reference" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: reference"
            }), 400
        
        # æ‰§è¡Œè¯„ä¼°
        score = evaluator.score(
            source=data.get("source", ""),
            translation=data["translation"],
            reference=data["reference"],
            mqm_score=data.get("mqm_score")
        )
        
        # è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¤„ç†dataclassï¼‰
        if isinstance(score, PaperGradeScore):
            score_dict = {
                "bleu": score.bleu,
                "comet": score.comet,
                "bleurt": score.bleurt,
                "bertscore_f1": score.bertscore_f1,
                "chrf": score.chrf,
                "mqm_adequacy": score.mqm_adequacy,
                "mqm_fluency": score.mqm_fluency,
                "mqm_terminology": score.mqm_terminology,
                "mqm_overall": score.mqm_overall,
                "final_score": score.final_score,
                "model_info": score.model_info
            }
        else:
            score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
        
        return jsonify({
            "success": True,
            "score": score_dict
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        print(f"è¯„ä¼°é”™è¯¯: {error_msg}")
        print(traceback_str)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


@app.route("/eval/batch", methods=["POST"])
def eval_batch():
    """
    æ‰¹é‡è¯„ä¼°
    
    Request Body:
    {
        "sources": ["æºæ–‡æœ¬1", "æºæ–‡æœ¬2", ...],
        "translations": ["ç¿»è¯‘1", "ç¿»è¯‘2", ...],
        "references": ["å‚è€ƒ1", "å‚è€ƒ2", ...],
        "mqm_scores": [
            {"overall": 0.9},
            {"overall": 0.85}
        ]  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "scores": [
            {
                "bleu": 0.85,
                "comet": 0.92,
                ...
            },
            ...
        ]
    }
    """
    try:
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translations" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translations"
            }), 400
        
        if "references" not in data:
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: references"
            }), 400
        
        translations = data["translations"]
        references = data["references"]
        sources = data.get("sources", [""] * len(translations))
        mqm_scores = data.get("mqm_scores", [None] * len(translations))
        
        # éªŒè¯é•¿åº¦
        if len(translations) != len(references):
            return jsonify({
                "success": False,
                "error": f"translationså’Œreferencesé•¿åº¦ä¸åŒ¹é…: {len(translations)} vs {len(references)}"
            }), 400
        
        # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
        results = evaluator.batch_score(
            sources=sources,
            translations=translations,
            references=references,
            mqm_scores=mqm_scores if mqm_scores else None
        )
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        scores_list = []
        for score in results:
            if isinstance(score, PaperGradeScore):
                score_dict = {
                    "bleu": score.bleu,
                    "comet": score.comet,
                    "bleurt": score.bleurt,
                    "bertscore_f1": score.bertscore_f1,
                    "chrf": score.chrf,
                    "mqm_adequacy": score.mqm_adequacy,
                    "mqm_fluency": score.mqm_fluency,
                    "mqm_terminology": score.mqm_terminology,
                    "mqm_overall": score.mqm_overall,
                    "final_score": score.final_score
                }
            else:
                score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
            scores_list.append(score_dict)
        
        return jsonify({
            "success": True,
            "count": len(scores_list),
            "scores": scores_list
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        print(f"æ‰¹é‡è¯„ä¼°é”™è¯¯: {error_msg}")
        print(traceback_str)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--port", type=int, default=5001, help="ç›‘å¬ç«¯å£ (é»˜è®¤: 5001)")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--use-bleurt", action="store_true", help="å¯ç”¨BLEURTè¯„ä¼°å™¨")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†ä½¿ç”¨BLEURTï¼Œæ›´æ–°é…ç½®
    if args.use_bleurt:
        print("âš ï¸  æ³¨æ„: ä½¿ç”¨BLEURTéœ€è¦TensorFlow")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    init_evaluator()
    
    print(f"\nğŸš€ å¯åŠ¨APIæœåŠ¡å™¨...")
    print(f"   åœ°å€: http://{args.host}:{args.port}")
    print(f"   è°ƒè¯•æ¨¡å¼: {args.debug}")
    print(f"\nğŸ“– APIæ–‡æ¡£: http://{args.host}:{args.port}/")
    print(f"ğŸ’š å¥åº·æ£€æŸ¥: http://{args.host}:{args.port}/health")
    print(f"ğŸ“Š è¯„ä¼°æ¥å£: http://{args.host}:{args.port}/eval")
    print(f"ğŸ“¦ æ‰¹é‡è¯„ä¼°: http://{args.host}:{args.port}/eval/batch")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    app.run(host=args.host, port=args.port, debug=args.debug)

