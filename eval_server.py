"""
ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨
æä¾›HTTP APIæ¥å£ï¼Œæ”¯æŒç‹¬ç«‹è¿è¡Œ
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
import traceback
import sys
import os
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from translation_evaluator import UnifiedEvaluator, PaperGradeScore

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# Debugæ¨¡å¼é…ç½®ï¼ˆé»˜è®¤å¼€å¯ï¼‰
DEBUG_MODE = True
LOGS_DIR = Path(__file__).parent / "logs"
LOGS_DIR.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—è®°å½•å™¨
def setup_logger():
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger('api_debug')
    logger.setLevel(logging.DEBUG)
    
    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger
    
    # åˆ›å»ºæ–‡ä»¶handlerï¼ŒæŒ‰æ—¥æœŸå‘½å
    log_file = LOGS_DIR / f"api_{datetime.now().strftime('%Y%m%d')}.log"
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    
    # åˆ›å»ºæ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    return logger

api_logger = setup_logger()

# å…¨å±€è¯„ä¼°å™¨å®ä¾‹å’Œé…ç½®
evaluator = None
evaluator_config = {
    "use_bleu": True,
    "use_comet": True,
    "use_bleurt": True,  # é»˜è®¤å…³é—­ï¼Œéœ€è¦TensorFlow
    "use_bertscore": True,
    "use_mqm": True,
    "use_chrf": True
}


def init_evaluator(use_bleurt=None, force_reinit=False):
    """
    åˆå§‹åŒ–è¯„ä¼°å™¨
    
    Args:
        use_bleurt: æ˜¯å¦ä½¿ç”¨BLEURTï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨å±€é…ç½®ï¼‰
        force_reinit: æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ï¼ˆå³ä½¿å·²åˆå§‹åŒ–ï¼‰
    """
    global evaluator, evaluator_config
    
    # å¦‚æœæŒ‡å®šäº†use_bleurtï¼Œæ›´æ–°é…ç½®
    if use_bleurt is not None:
        evaluator_config["use_bleurt"] = use_bleurt
        # å¦‚æœé…ç½®æ”¹å˜ä¸”è¯„ä¼°å™¨å·²åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
        if evaluator is not None:
            force_reinit = True
    
    if evaluator is None or force_reinit:
        if force_reinit and evaluator is not None:
            print("âš ï¸  æ£€æµ‹åˆ°é…ç½®å˜æ›´ï¼Œé‡æ–°åˆå§‹åŒ–è¯„ä¼°å™¨...")
            evaluator = None
        print("=" * 80)
        print("åˆå§‹åŒ–ç¿»è¯‘è¯„ä¼°å™¨...")
        print("=" * 80)
        
        if evaluator_config["use_bleurt"]:
            print("âš ï¸  å¯ç”¨BLEURTè¯„ä¼°å™¨ï¼ˆéœ€è¦TensorFlowå’Œæ¨¡å‹æ–‡ä»¶ï¼‰")
        
        evaluator = UnifiedEvaluator(
            use_bleu=evaluator_config["use_bleu"],
            use_comet=evaluator_config["use_comet"],
            use_bleurt=evaluator_config["use_bleurt"],
            use_bertscore=evaluator_config["use_bertscore"],
            use_mqm=evaluator_config["use_mqm"],
            use_chrf=evaluator_config["use_chrf"]
        )
        
        success = evaluator.initialize()
        
        # æ˜¾ç¤ºå®é™…å¯ç”¨çš„è¯„ä¼°å™¨çŠ¶æ€
        print("\n" + "=" * 80)
        print("è¯„ä¼°å™¨çŠ¶æ€:")
        print("=" * 80)
        
        enabled = []
        failed = []
        
        if evaluator_config["use_bleu"]:
            enabled.append("BLEU")
        
        if evaluator_config["use_comet"]:
            if evaluator.use_comet and evaluator.comet_scorer:
                enabled.append("COMET âœ…")
            else:
                failed.append("COMET âŒ")
        
        if evaluator_config["use_bleurt"]:
            if evaluator.use_bleurt and evaluator.bleurt_scorer:
                enabled.append("BLEURT âœ…")
            else:
                failed.append("BLEURT âŒ (å¯èƒ½ç¼ºå°‘TensorFlowæˆ–æ¨¡å‹æ–‡ä»¶)")
        
        if evaluator_config["use_bertscore"]:
            if evaluator.use_bertscore and evaluator.bertscore_scorer:
                enabled.append("BERTScore âœ…")
            else:
                failed.append("BERTScore âŒ")
        
        if evaluator_config["use_chrf"]:
            if evaluator.use_chrf and evaluator.chrf_scorer:
                enabled.append("ChrF âœ…")
            else:
                failed.append("ChrF âŒ")
        
        if evaluator_config["use_mqm"]:
            enabled.append("MQM")
        
        if enabled:
            print(f"âœ… å·²å¯ç”¨: {', '.join(enabled)}")
        if failed:
            print(f"âš ï¸  åˆå§‹åŒ–å¤±è´¥: {', '.join(failed)}")
        
        if success:
            print("\nâœ… è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼")
        else:
            print("\nâš ï¸  éƒ¨åˆ†è¯„ä¼°å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œä½†æœåŠ¡ä»å¯è¿è¡Œ")
        
        print("=" * 80)
        print("APIæœåŠ¡å·²å°±ç»ª")
        print("=" * 80)
    
    return evaluator


@app.route("/", methods=["GET"])
def index():
    """APIé¦–é¡µ"""
    return jsonify({
        "service": "Translation Evaluator API",
        "version": "1.0.0",
        "endpoints": {
            "/": "APIä¿¡æ¯",
            "/health": "å¥åº·æ£€æŸ¥",
            "/eval": "å•ä¸ªæ ·æœ¬è¯„ä¼° (POST)",
            "/eval/batch": "æ‰¹é‡è¯„ä¼° (POST)"
        },
        "usage": {
            "single": {
                "url": "/eval",
                "method": "POST",
                "body": {
                    "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
                    "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
                    "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
                    "mqm_score": "MQMè¯„åˆ†ï¼ˆå¯é€‰ï¼‰"
                }
            },
            "batch": {
                "url": "/eval/batch",
                "method": "POST",
                "body": {
                    "sources": ["æºæ–‡æœ¬åˆ—è¡¨"],
                    "translations": ["ç¿»è¯‘æ–‡æœ¬åˆ—è¡¨"],
                    "references": ["å‚è€ƒç¿»è¯‘åˆ—è¡¨"],
                    "mqm_scores": ["MQMè¯„åˆ†åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰"]
                }
            }
        }
    })


@app.route("/health", methods=["GET"])
def health():
    """å¥åº·æ£€æŸ¥"""
    evaluator_status = {}
    if evaluator is not None:
        evaluator_status = {
            "use_bleu": evaluator.use_bleu,
            "use_comet": evaluator.use_comet and evaluator.comet_scorer is not None,
            "use_bleurt": evaluator.use_bleurt and evaluator.bleurt_scorer is not None,
            "use_bertscore": evaluator.use_bertscore and evaluator.bertscore_scorer is not None,
            "use_chrf": evaluator.use_chrf and evaluator.chrf_scorer is not None,
            "use_mqm": evaluator.use_mqm
        }
    
    return jsonify({
        "status": "healthy",
        "evaluator_initialized": evaluator is not None,
        "evaluator_status": evaluator_status
    })


@app.route("/eval", methods=["POST"])
def eval_text():
    """
    å•ä¸ªæ ·æœ¬è¯„ä¼°
    
    Request Body:
    {
        "source": "æºæ–‡æœ¬ï¼ˆå¯é€‰ï¼‰",
        "translation": "ç¿»è¯‘æ–‡æœ¬ï¼ˆå¿…éœ€ï¼‰",
        "reference": "å‚è€ƒç¿»è¯‘ï¼ˆå¿…éœ€ï¼‰",
        "mqm_score": {
            "adequacy": 0.9,
            "fluency": 0.85,
            "terminology": 0.95,
            "overall": 0.9
        }  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "score": {
            "bleu": 0.85,
            "comet": 0.92,
            "bleurt": 0.0,
            "bertscore_f1": 0.88,
            "chrf": 0.87,
            "mqm_adequacy": 0.9,
            "mqm_fluency": 0.85,
            "mqm_terminology": 0.95,
            "mqm_overall": 0.9,
            "final_score": 0.89
        }
    }
    """
    request_id = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    
    try:
        if DEBUG_MODE:
            api_logger.info("=" * 100)
            api_logger.info(f"ğŸ“¥ [è¯·æ±‚ID: {request_id}] æ”¶åˆ°å•ä¸ªæ ·æœ¬è¯„ä¼°è¯·æ±‚")
            api_logger.info("=" * 100)
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] è¯·æ±‚ä½“ä¸ºç©º")
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # è®°å½•è¯·æ±‚æ•°æ®
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“‹ è¯·æ±‚æ•°æ®:")
            api_logger.info(f"  - translationé•¿åº¦: {len(data.get('translation', ''))}")
            api_logger.info(f"  - referenceé•¿åº¦: {len(data.get('reference', ''))}")
            api_logger.info(f"  - sourceé•¿åº¦: {len(data.get('source', ''))}")
            api_logger.info(f"  - æ˜¯å¦æœ‰mqm_score: {'mqm_score' in data and data['mqm_score'] is not None}")
            if DEBUG_MODE:
                # è®°å½•å®Œæ•´æ•°æ®ï¼ˆæˆªæ–­é•¿æ–‡æœ¬ï¼‰
                log_data = data.copy()
                for key in ['translation', 'reference', 'source']:
                    if key in log_data and len(log_data[key]) > 200:
                        log_data[key] = log_data[key][:200] + f"... (æ€»é•¿åº¦: {len(data[key])})"
                api_logger.debug(f"[è¯·æ±‚ID: {request_id}] å®Œæ•´è¯·æ±‚æ•°æ®: {json.dumps(log_data, ensure_ascii=False, indent=2)}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translation" not in data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] ç¼ºå°‘å¿…éœ€å­—æ®µ: translation")
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translation"
            }), 400
        
        if "reference" not in data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] ç¼ºå°‘å¿…éœ€å­—æ®µ: reference")
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: reference"
            }), 400
        
        # æ‰§è¡Œè¯„ä¼°
        reference = data["reference"]
        translation = data["translation"]
        source = data.get("source", "")
        mqm_score = data.get("mqm_score")
        
        # è®°å½•è¯„ä¼°å™¨çŠ¶æ€
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ”§ è¯„ä¼°å™¨çŠ¶æ€:")
            api_logger.info(f"  - use_bleu: {evaluator.use_bleu}")
            api_logger.info(f"  - use_comet: {evaluator.use_comet}")
            api_logger.info(f"  - use_bleurt: {evaluator.use_bleurt}")
            api_logger.info(f"  - use_bertscore: {evaluator.use_bertscore}")
            api_logger.info(f"  - use_chrf: {evaluator.use_chrf}")
            api_logger.info(f"  - use_mqm: {evaluator.use_mqm}")
            api_logger.info(f"  - COMETè¯„ä¼°å™¨å­˜åœ¨: {evaluator.comet_scorer is not None}")
            api_logger.info(f"  - BLEURTè¯„ä¼°å™¨å­˜åœ¨: {evaluator.bleurt_scorer is not None}")
            api_logger.info(f"  - BERTScoreè¯„ä¼°å™¨å­˜åœ¨: {evaluator.bertscore_scorer is not None}")
            api_logger.info(f"  - ChrFè¯„ä¼°å™¨å­˜åœ¨: {evaluator.chrf_scorer is not None}")
        
        # éªŒè¯referenceä¸ä¸ºç©º
        if not reference or not reference.strip():
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] referenceä¸ºç©º")
            return jsonify({
                "success": False,
                "error": "referenceä¸èƒ½ä¸ºç©ºï¼ˆBLEURTç­‰è¯„ä¼°å™¨éœ€è¦referenceï¼‰"
            }), 400
        
        # å¼€å§‹è¯„ä¼°
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸš€ å¼€å§‹è¯„ä¼°...")
            start_time = datetime.now()
        
        score = evaluator.score(
            source=source,
            translation=translation,
            reference=reference,
            mqm_score=mqm_score
        )
        
        # è®°å½•è¯„ä¼°ç»“æœ
        if DEBUG_MODE:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] âœ… è¯„ä¼°å®Œæˆ (è€—æ—¶: {duration:.3f}ç§’)")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“Š è¯„ä¼°ç»“æœ:")
            api_logger.info(f"  - BLEU: {score.bleu:.6f}")
            api_logger.info(f"  - COMET: {score.comet:.6f}")
            api_logger.info(f"  - BLEURT: {score.bleurt:.6f}")
            api_logger.info(f"  - BERTScore F1: {score.bertscore_f1:.6f}")
            api_logger.info(f"  - ChrF: {score.chrf:.6f}")
            api_logger.info(f"  - MQM Adequacy: {score.mqm_adequacy:.6f}")
            api_logger.info(f"  - MQM Fluency: {score.mqm_fluency:.6f}")
            api_logger.info(f"  - MQM Terminology: {score.mqm_terminology:.6f}")
            api_logger.info(f"  - MQM Overall: {score.mqm_overall:.6f}")
            api_logger.info(f"  - ç»¼åˆè¯„åˆ†: {score.final_score:.6f}")
            if hasattr(score, 'model_info') and score.model_info:
                api_logger.info(f"  - æ¨¡å‹ä¿¡æ¯: {score.model_info}")
        
        # è½¬æ¢ä¸ºå­—å…¸ï¼ˆå¤„ç†dataclassï¼‰
        if isinstance(score, PaperGradeScore):
            score_dict = {
                "bleu": score.bleu,
                "comet": score.comet,
                "bleurt": score.bleurt,  # ç¡®ä¿BLEURTæ€»æ˜¯åŒ…å«åœ¨è¿”å›ç»“æœä¸­
                "bertscore_f1": score.bertscore_f1,
                "chrf": score.chrf,
                "mqm_adequacy": score.mqm_adequacy,
                "mqm_fluency": score.mqm_fluency,
                "mqm_terminology": score.mqm_terminology,
                "mqm_overall": score.mqm_overall,
                "final_score": score.final_score,
                "model_info": score.model_info
            }
            # è°ƒè¯•ä¿¡æ¯ï¼šå¦‚æœBLEURTä¸º0ä½†è¯„ä¼°å™¨å·²å¯ç”¨ï¼Œè®°å½•æ—¥å¿—
            if evaluator.use_bleurt and score.bleurt == 0.0:
                if DEBUG_MODE:
                    api_logger.warning(f"[è¯·æ±‚ID: {request_id}] âš ï¸  BLEURTå·²å¯ç”¨ä½†åˆ†æ•°ä¸º0")
        else:
            score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
            # ç¡®ä¿BLEURTå­—æ®µå­˜åœ¨
            if "bleurt" not in score_dict:
                score_dict["bleurt"] = 0.0
        
        # è®°å½•è¿”å›ç»“æœ
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“¤ è¿”å›ç»“æœ:")
            api_logger.debug(f"[è¯·æ±‚ID: {request_id}] å®Œæ•´è¿”å›æ•°æ®: {json.dumps(score_dict, ensure_ascii=False, indent=2)}")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] " + "=" * 100)
        
        return jsonify({
            "success": True,
            "score": score_dict
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        
        if DEBUG_MODE:
            api_logger.error(f"[è¯·æ±‚ID: {request_id}] âŒ è¯„ä¼°é”™è¯¯: {error_msg}")
            api_logger.error(f"[è¯·æ±‚ID: {request_id}] é”™è¯¯å †æ ˆ:\n{traceback_str}")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] " + "=" * 100)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


@app.route("/eval/batch", methods=["POST"])
def eval_batch():
    """
    æ‰¹é‡è¯„ä¼°
    
    Request Body:
    {
        "sources": ["æºæ–‡æœ¬1", "æºæ–‡æœ¬2", ...],
        "translations": ["ç¿»è¯‘1", "ç¿»è¯‘2", ...],
        "references": ["å‚è€ƒ1", "å‚è€ƒ2", ...],
        "mqm_scores": [
            {"overall": 0.9},
            {"overall": 0.85}
        ]  // å¯é€‰
    }
    
    Response:
    {
        "success": true,
        "scores": [
            {
                "bleu": 0.85,
                "comet": 0.92,
                ...
            },
            ...
        ]
    }
    """
    request_id = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    
    try:
        if DEBUG_MODE:
            api_logger.info("=" * 100)
            api_logger.info(f"ğŸ“¥ [è¯·æ±‚ID: {request_id}] æ”¶åˆ°æ‰¹é‡è¯„ä¼°è¯·æ±‚")
            api_logger.info("=" * 100)
        # ç¡®ä¿è¯„ä¼°å™¨å·²åˆå§‹åŒ–
        if evaluator is None:
            init_evaluator()
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.json
        if not data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] è¯·æ±‚ä½“ä¸ºç©º")
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # è®°å½•è¯·æ±‚æ•°æ®
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“‹ è¯·æ±‚æ•°æ®:")
            api_logger.info(f"  - translationsæ•°é‡: {len(data.get('translations', []))}")
            api_logger.info(f"  - referencesæ•°é‡: {len(data.get('references', []))}")
            api_logger.info(f"  - sourcesæ•°é‡: {len(data.get('sources', []))}")
            api_logger.info(f"  - mqm_scoresæ•°é‡: {len(data.get('mqm_scores', []))}")
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if "translations" not in data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] ç¼ºå°‘å¿…éœ€å­—æ®µ: translations")
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: translations"
            }), 400
        
        if "references" not in data:
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] ç¼ºå°‘å¿…éœ€å­—æ®µ: references")
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘å¿…éœ€å­—æ®µ: references"
            }), 400
        
        translations = data["translations"]
        references = data["references"]
        sources = data.get("sources", [""] * len(translations))
        mqm_scores = data.get("mqm_scores", [None] * len(translations))
        
        # éªŒè¯é•¿åº¦
        if len(translations) != len(references):
            if DEBUG_MODE:
                api_logger.error(f"[è¯·æ±‚ID: {request_id}] é•¿åº¦ä¸åŒ¹é…: translations={len(translations)}, references={len(references)}")
            return jsonify({
                "success": False,
                "error": f"translationså’Œreferencesé•¿åº¦ä¸åŒ¹é…: {len(translations)} vs {len(references)}"
            }), 400
        
        # è®°å½•è¯„ä¼°å™¨çŠ¶æ€
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ”§ è¯„ä¼°å™¨çŠ¶æ€:")
            api_logger.info(f"  - use_bleu: {evaluator.use_bleu}")
            api_logger.info(f"  - use_comet: {evaluator.use_comet}")
            api_logger.info(f"  - use_bleurt: {evaluator.use_bleurt}")
            api_logger.info(f"  - use_bertscore: {evaluator.use_bertscore}")
            api_logger.info(f"  - use_chrf: {evaluator.use_chrf}")
            api_logger.info(f"  - use_mqm: {evaluator.use_mqm}")
        
        # å¼€å§‹æ‰¹é‡è¯„ä¼°
        if DEBUG_MODE:
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°...")
            start_time = datetime.now()
        
        results = evaluator.batch_score(
            sources=sources,
            translations=translations,
            references=references,
            mqm_scores=mqm_scores if mqm_scores else None
        )
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        scores_list = []
        for i, score in enumerate(results):
            if isinstance(score, PaperGradeScore):
                score_dict = {
                    "bleu": score.bleu,
                    "comet": score.comet,
                    "bleurt": score.bleurt,
                    "bertscore_f1": score.bertscore_f1,
                    "chrf": score.chrf,
                    "mqm_adequacy": score.mqm_adequacy,
                    "mqm_fluency": score.mqm_fluency,
                    "mqm_terminology": score.mqm_terminology,
                    "mqm_overall": score.mqm_overall,
                    "final_score": score.final_score
                }
            else:
                score_dict = score.__dict__ if hasattr(score, '__dict__') else {}
            scores_list.append(score_dict)
            
            # è®°å½•æ¯ä¸ªæ ·æœ¬çš„è¯„ä¼°ç»“æœ
            if DEBUG_MODE:
                api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“Š æ ·æœ¬ {i+1}/{len(results)} è¯„ä¼°ç»“æœ:")
                api_logger.info(f"  - BLEU: {score_dict.get('bleu', 0):.6f}")
                api_logger.info(f"  - COMET: {score_dict.get('comet', 0):.6f}")
                api_logger.info(f"  - BLEURT: {score_dict.get('bleurt', 0):.6f}")
                api_logger.info(f"  - BERTScore F1: {score_dict.get('bertscore_f1', 0):.6f}")
                api_logger.info(f"  - ChrF: {score_dict.get('chrf', 0):.6f}")
                api_logger.info(f"  - ç»¼åˆè¯„åˆ†: {score_dict.get('final_score', 0):.6f}")
        
        if DEBUG_MODE:
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ (æ€»è€—æ—¶: {duration:.3f}ç§’, å¹³å‡: {duration/len(results):.3f}ç§’/æ ·æœ¬)")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] ğŸ“¤ è¿”å› {len(scores_list)} ä¸ªè¯„ä¼°ç»“æœ")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] " + "=" * 100)
        
        return jsonify({
            "success": True,
            "count": len(scores_list),
            "scores": scores_list
        })
        
    except Exception as e:
        error_msg = str(e)
        traceback_str = traceback.format_exc()
        
        if DEBUG_MODE:
            api_logger.error(f"[è¯·æ±‚ID: {request_id}] âŒ æ‰¹é‡è¯„ä¼°é”™è¯¯: {error_msg}")
            api_logger.error(f"[è¯·æ±‚ID: {request_id}] é”™è¯¯å †æ ˆ:\n{traceback_str}")
            api_logger.info(f"[è¯·æ±‚ID: {request_id}] " + "=" * 100)
        
        return jsonify({
            "success": False,
            "error": error_msg,
            "traceback": traceback_str if app.debug else None
        }), 500


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ç¿»è¯‘è¯„ä¼°APIæœåŠ¡å™¨")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--port", type=int, default=5001, help="ç›‘å¬ç«¯å£ (é»˜è®¤: 5001)")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨Flaskè°ƒè¯•æ¨¡å¼")
    parser.add_argument("--use-bleurt", action="store_true", help="å¯ç”¨BLEURTè¯„ä¼°å™¨")
    parser.add_argument("--no-api-debug", action="store_true", help="ç¦ç”¨APIè¯·æ±‚è°ƒè¯•æ—¥å¿—ï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®DEBUG_MODE
    DEBUG_MODE = not args.no_api_debug
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆä¼ é€’use_bleurtå‚æ•°ï¼‰
    # å¦‚æœå‘½ä»¤è¡ŒæŒ‡å®šäº†--use-bleurtï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼›å¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
    use_bleurt = args.use_bleurt if args.use_bleurt else evaluator_config.get("use_bleurt", False)
    print(f"\nğŸ” [DEBUG] BLEURTé…ç½®:")
    print(f"   å‘½ä»¤è¡Œå‚æ•° --use-bleurt: {args.use_bleurt}")
    print(f"   é…ç½®ä¸­çš„ use_bleurt: {evaluator_config.get('use_bleurt', False)}")
    print(f"   æœ€ç»ˆä½¿ç”¨: {use_bleurt}")
    init_evaluator(use_bleurt=use_bleurt)
    
    print(f"\nğŸš€ å¯åŠ¨APIæœåŠ¡å™¨...")
    print(f"   åœ°å€: http://{args.host}:{args.port}")
    print(f"   Flaskè°ƒè¯•æ¨¡å¼: {args.debug}")
    print(f"   APIè¯·æ±‚è°ƒè¯•æ—¥å¿—: {'å¼€å¯' if DEBUG_MODE else 'å…³é—­'}")
    print(f"   æ—¥å¿—ç›®å½•: {LOGS_DIR}")
    if DEBUG_MODE:
        log_file = LOGS_DIR / f"api_{datetime.now().strftime('%Y%m%d')}.log"
        print(f"   æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"\nğŸ“– APIæ–‡æ¡£: http://{args.host}:{args.port}/")
    print(f"ğŸ’š å¥åº·æ£€æŸ¥: http://{args.host}:{args.port}/health")
    print(f"ğŸ“Š è¯„ä¼°æ¥å£: http://{args.host}:{args.port}/eval")
    print(f"ğŸ“¦ æ‰¹é‡è¯„ä¼°: http://{args.host}:{args.port}/eval/batch")
    print("\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨\n")
    
    app.run(host=args.host, port=args.port, debug=args.debug)

