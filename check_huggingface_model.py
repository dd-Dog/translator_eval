#!/usr/bin/env python
"""
æ£€æŸ¥HuggingFaceæ¨¡å‹æ˜¯å¦æ­£ç¡®é…ç½®
"""

import os
import sys

def check_huggingface_model():
    """æ£€æŸ¥xlm-roberta-largeæ¨¡å‹é…ç½®"""
    print("=" * 80)
    print("HuggingFaceæ¨¡å‹é…ç½®æ£€æŸ¥")
    print("=" * 80)
    
    # 1. æ£€æŸ¥ç¯å¢ƒå˜é‡
    print("\n1. ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    hf_home = os.environ.get("HF_HOME")
    transformers_cache = os.environ.get("TRANSFORMERS_CACHE")
    hf_hub_offline = os.environ.get("HF_HUB_OFFLINE")
    transformers_offline = os.environ.get("TRANSFORMERS_OFFLINE")
    
    print(f"   HF_HOME: {hf_home or '(æœªè®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤)'}")
    print(f"   TRANSFORMERS_CACHE: {transformers_cache or '(æœªè®¾ç½®)'}")
    print(f"   HF_HUB_OFFLINE: {hf_hub_offline or '(æœªè®¾ç½®)'}")
    print(f"   TRANSFORMERS_OFFLINE: {transformers_offline or '(æœªè®¾ç½®)'}")
    
    # 2. ç¡®å®šæ¨¡å‹è·¯å¾„
    if hf_home:
        hf_cache = os.path.join(hf_home, "hub")
    elif transformers_cache:
        hf_cache = transformers_cache
    else:
        hf_cache = os.path.expanduser("~/.cache/huggingface/hub")
    
    xlm_model_path = os.path.join(hf_cache, "models--xlm-roberta-large")
    
    print(f"\n2. æ¨¡å‹è·¯å¾„æ£€æŸ¥:")
    print(f"   HuggingFaceç¼“å­˜ç›®å½•: {hf_cache}")
    print(f"   xlm-roberta-largeè·¯å¾„: {xlm_model_path}")
    print(f"   è·¯å¾„å­˜åœ¨: {os.path.exists(xlm_model_path)}")
    
    if not os.path.exists(xlm_model_path):
        print(f"\n   âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ï¼")
        print(f"\n   ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. åœ¨æœ‰å¤–ç½‘çš„æœºå™¨ä¸Šä¸‹è½½æ¨¡å‹:")
        print(f"      python -c \"from huggingface_hub import snapshot_download; snapshot_download('xlm-roberta-large', cache_dir='./models')\"")
        print(f"\n   2. ä¼ è¾“åˆ°æœåŠ¡å™¨:")
        print(f"      scp -r ./models/models--xlm-roberta-large root@server:{hf_cache}/")
        return False
    
    # 3. æ£€æŸ¥snapshotsç›®å½•
    print(f"\n3. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶:")
    snapshots_dir = os.path.join(xlm_model_path, "snapshots")
    
    # æ£€æŸ¥æ ¹ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼ˆå¯èƒ½æ–‡ä»¶ç›´æ¥åœ¨models--xlm-roberta-largeä¸‹ï¼‰
    root_files = []
    if os.path.exists(xlm_model_path):
        try:
            root_files = os.listdir(xlm_model_path)
            print(f"   æ ¹ç›®å½•å†…å®¹: {root_files[:10]}")
        except:
            pass
    
    # æ£€æŸ¥snapshotsç›®å½•
    if os.path.exists(snapshots_dir):
        snapshots = os.listdir(snapshots_dir)
        if snapshots:
            snapshot_path = os.path.join(snapshots_dir, snapshots[0])
            print(f"   âœ… æ‰¾åˆ°snapshotsç›®å½•")
            print(f"   å¿«ç…§è·¯å¾„: {snapshot_path}")
        else:
            print(f"   âš ï¸  snapshotsç›®å½•ä¸ºç©º")
            snapshot_path = None
    else:
        print(f"   âš ï¸  snapshotsç›®å½•ä¸å­˜åœ¨")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç›´æ¥åœ¨æ ¹ç›®å½•
        if root_files:
            # æŸ¥æ‰¾tokenizeræ–‡ä»¶
            tokenizer_files = [f for f in root_files if any(x in f.lower() for x in ['tokenizer', 'vocab', 'merges', 'sentencepiece'])]
            if tokenizer_files:
                print(f"   ğŸ’¡ å‘ç°tokenizeræ–‡ä»¶åœ¨æ ¹ç›®å½•: {tokenizer_files}")
                snapshot_path = xlm_model_path  # ä½¿ç”¨æ ¹ç›®å½•ä½œä¸ºå¿«ç…§è·¯å¾„
            else:
                print(f"   âŒ æœªæ‰¾åˆ°tokenizeræ–‡ä»¶")
                snapshot_path = None
        else:
            snapshot_path = None
    
    if not snapshot_path:
        print(f"\n   âŒ æ— æ³•æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        print(f"\n   ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. å¦‚æœæ–‡ä»¶åœ¨æ ¹ç›®å½•ï¼Œéœ€è¦åˆ›å»ºsnapshotsç»“æ„")
        print(f"   2. æˆ–è€…é‡æ–°ä¸‹è½½æ¨¡å‹åˆ°æ­£ç¡®ä½ç½®")
        return False
    
    # 4. æ£€æŸ¥å¿…éœ€æ–‡ä»¶
    # å¯¹äºsentencepiece tokenizerï¼Œvocab.jsonå’Œmerges.txtä¸æ˜¯å¿…éœ€çš„
    core_files = [
        "tokenizer_config.json",
        "tokenizer.json",
    ]
    
    # æ£€æŸ¥tokenizerç±»å‹
    tokenizer_type = None
    has_sentencepiece = False
    has_bpe = False
    
    if os.path.exists(snapshot_path):
        all_files = os.listdir(snapshot_path)
        has_sentencepiece = any('sentencepiece' in f.lower() or '.bpe.model' in f.lower() for f in all_files)
        has_bpe = any('vocab.json' in f.lower() or 'merges.txt' in f.lower() for f in all_files)
        
        # è¯»å–tokenizer_config.jsonç¡®å®šç±»å‹
        tokenizer_config_path = os.path.join(snapshot_path, "tokenizer_config.json")
        if os.path.exists(tokenizer_config_path):
            try:
                import json
                with open(tokenizer_config_path, 'r') as f:
                    config = json.load(f)
                    tokenizer_type = config.get("tokenizer_class", "").lower()
            except:
                pass
    
    print(f"\n4. æ£€æŸ¥å¿…éœ€æ–‡ä»¶ (è·¯å¾„: {snapshot_path}):")
    print(f"   Tokenizerç±»å‹: {tokenizer_type or 'æœªçŸ¥'}")
    print(f"   æœ‰sentencepieceæ–‡ä»¶: {has_sentencepiece}")
    print(f"   æœ‰BPEæ–‡ä»¶: {has_bpe}")
    
    found_files = []
    missing_core = []
    
    # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶
    for file in core_files:
        file_path = os.path.join(snapshot_path, file)
        if os.path.exists(file_path):
            print(f"   âœ… {file}")
            found_files.append(file)
        else:
            print(f"   âŒ {file} (ç¼ºå¤±)")
            missing_core.append(file)
    
    # æ£€æŸ¥tokenizeræ¨¡å‹æ–‡ä»¶
    if has_sentencepiece:
        sp_files = [f for f in os.listdir(snapshot_path) if 'sentencepiece' in f.lower() or '.bpe.model' in f.lower()]
        if sp_files:
            print(f"   âœ… SentencePieceæ¨¡å‹: {sp_files[0]}")
            found_files.append("sentencepiece")
        else:
            print(f"   âš ï¸  SentencePieceæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°")
    elif has_bpe:
        if os.path.exists(os.path.join(snapshot_path, "vocab.json")):
            print(f"   âœ… vocab.json")
            found_files.append("vocab.json")
        else:
            print(f"   âš ï¸  vocab.json (BPE tokenizerå¯èƒ½éœ€è¦)")
        
        if os.path.exists(os.path.join(snapshot_path, "merges.txt")):
            print(f"   âœ… merges.txt")
            found_files.append("merges.txt")
        else:
            print(f"   âš ï¸  merges.txt (BPE tokenizerå¯èƒ½éœ€è¦)")
    
    if missing_core:
        print(f"\n   âŒ ç¼ºå°‘æ ¸å¿ƒæ–‡ä»¶: {', '.join(missing_core)}")
        return False
    
    # å¯¹äºsentencepiece tokenizerï¼Œvocab.jsonå’Œmerges.txtä¸æ˜¯å¿…éœ€çš„
    if has_sentencepiece and len(found_files) >= 2:
        print(f"\n   âœ… æ ¸å¿ƒæ–‡ä»¶å®Œæ•´ (sentencepiece tokenizerä¸éœ€è¦vocab.jsonå’Œmerges.txt)")
        return True
    elif has_bpe and len(found_files) >= 4:
        print(f"\n   âœ… æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨")
        return True
    elif len(found_files) >= 2:
        print(f"\n   âš ï¸  æ‰¾åˆ° {len(found_files)} ä¸ªæ–‡ä»¶ï¼Œå°è¯•åŠ è½½æµ‹è¯•")
        return True
    else:
        print(f"\n   âŒ æ–‡ä»¶ä¸å®Œæ•´")
        return False
    
    # 5. è®¾ç½®ç¦»çº¿æ¨¡å¼
    print(f"\n5. è®¾ç½®ç¦»çº¿æ¨¡å¼:")
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    print(f"   âœ… å·²è®¾ç½®ç¦»çº¿æ¨¡å¼")
    
    # 6. æµ‹è¯•å¯¼å…¥
    print(f"\n6. æµ‹è¯•transformersåº“:")
    try:
        from transformers import AutoTokenizer
        print(f"   âœ… transformersåº“å·²å®‰è£…")
        
        # å°è¯•åŠ è½½tokenizer
        print(f"\n7. æµ‹è¯•åŠ è½½tokenizer:")
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨æ¨¡å‹åç§°ï¼ˆå¦‚æœsnapshotsç»“æ„æ­£ç¡®ï¼‰
            if "snapshots" in snapshot_path:
                print(f"   å°è¯•ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½...")
                tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-large", local_files_only=True)
                print(f"   âœ… tokenizeråŠ è½½æˆåŠŸï¼(ä½¿ç”¨æ¨¡å‹åç§°)")
                return True
            else:
                # å¦‚æœæ–‡ä»¶åœ¨æ ¹ç›®å½•ï¼Œç›´æ¥ä½¿ç”¨è·¯å¾„
                print(f"   å°è¯•ä½¿ç”¨è·¯å¾„åŠ è½½: {snapshot_path}")
                tokenizer = AutoTokenizer.from_pretrained(snapshot_path, local_files_only=True)
                print(f"   âœ… tokenizeråŠ è½½æˆåŠŸï¼(ä½¿ç”¨è·¯å¾„)")
                return True
        except Exception as e:
            error_msg = str(e)
            print(f"   âš ï¸  ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½å¤±è´¥: {error_msg[:200]}")
            print(f"\n   å°è¯•ä½¿ç”¨ç›´æ¥è·¯å¾„...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(snapshot_path, local_files_only=True)
                print(f"   âœ… tokenizeråŠ è½½æˆåŠŸï¼(ä½¿ç”¨ç›´æ¥è·¯å¾„)")
                return True
            except Exception as e2:
                error_msg2 = str(e2)
                print(f"   âš ï¸  ç›´æ¥è·¯å¾„ä¹Ÿå¤±è´¥: {error_msg2[:200]}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç½‘ç»œé—®é¢˜
                if "network" in error_msg2.lower() or "connection" in error_msg2.lower() or "unreachable" in error_msg2.lower():
                    print(f"\n   ğŸ’¡ æ£€æµ‹åˆ°ç½‘ç»œé”™è¯¯ï¼Œä½†æ–‡ä»¶ç»“æ„æ­£ç¡®")
                    print(f"   æç¤º: ç¡®ä¿è®¾ç½®äº†ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡")
                    print(f"   å½“å‰è®¾ç½®: HF_HUB_OFFLINE={os.environ.get('HF_HUB_OFFLINE')}")
                    print(f"   æ–‡ä»¶è·¯å¾„: {snapshot_path}")
                    # å³ä½¿æœ‰ç½‘ç»œé”™è¯¯ï¼Œå¦‚æœæ–‡ä»¶ç»“æ„æ­£ç¡®ï¼Œå¯èƒ½åœ¨å®é™…ä½¿ç”¨æ—¶å¯ä»¥å·¥ä½œ
                    if os.path.exists(os.path.join(snapshot_path, "tokenizer_config.json")):
                        print(f"   âš ï¸  æ–‡ä»¶ç»“æ„æ­£ç¡®ï¼Œä½†åŠ è½½æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼‰")
                        print(f"   å»ºè®®: åœ¨å®é™…ä½¿ç”¨COMETæ—¶è®¾ç½®ç¯å¢ƒå˜é‡åå†æµ‹è¯•")
                        return True  # æ–‡ä»¶ç»“æ„æ­£ç¡®ï¼Œè¿”å›True
                
                print(f"\n   ğŸ’¡ æç¤º:")
                print(f"   1. ç¡®ä¿è®¾ç½®äº†ç¦»çº¿æ¨¡å¼: export HF_HUB_OFFLINE=1")
                print(f"   2. ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´")
                print(f"   3. æ£€æŸ¥æ–‡ä»¶è·¯å¾„: {snapshot_path}")
                return False
    except ImportError:
        print(f"   âŒ transformersåº“æœªå®‰è£…")
        return False

if __name__ == "__main__":
    success = check_huggingface_model()
    print("\n" + "=" * 80)
    if success:
        print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼HuggingFaceæ¨¡å‹é…ç½®æ­£ç¡®ã€‚")
        print("\nğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œ:")
        print("   export HF_HUB_OFFLINE=1")
        print("   export TRANSFORMERS_OFFLINE=1")
        print("   python test_comet_path.py")
    else:
        print("âŒ æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŒ‰ç…§æç¤ºä¿®å¤é—®é¢˜ã€‚")
    print("=" * 80)
    sys.exit(0 if success else 1)
